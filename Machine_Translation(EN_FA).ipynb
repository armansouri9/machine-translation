{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# one"
      ],
      "metadata": {
        "id": "8rQoioOKxh1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_1gNlZXtxTj",
        "outputId": "5c709880-388c-49f0-eaa4-a85c594df207"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "avy_4Vp2s2a4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset\n",
        "input_texts = [\n",
        "    \"How are you?\",\n",
        "    \"What's your name?\",\n",
        "    \"What do you do?\",\n",
        "    \"Ali?\"\n",
        "]\n",
        "\n",
        "output_texts = [\n",
        "     \"I'm fine, thank you.\",\n",
        "    \"My name is John.\",\n",
        "    \"i'm engineer\",\n",
        "    \"Hello\"\n",
        "]"
      ],
      "metadata": {
        "id": "YVYQBPIlhOEI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# global max_len\n",
        "max_len= max(max(len(seq) for seq in input_texts), max(len(seq) for seq in output_texts))\n",
        "\n",
        "class Translator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Translator, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.encoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "\n",
        "        embedded = self.embedding(input_seq)\n",
        "        encoder_output, (encoder_hidden, encoder_cell) = self.encoder(embedded)\n",
        "        \n",
        "        decoder_output, _ = self.decoder(encoder_output, (encoder_hidden, encoder_cell))\n",
        "\n",
        "        output = self.fc(decoder_output)\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, input_texts, output_texts,max_len):\n",
        "        self.input_texts = input_texts\n",
        "        self.output_texts = output_texts\n",
        "        self.max_len=max_len\n",
        "    def __len__(self):\n",
        "        return len(self.input_texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_text = self.input_texts[index]\n",
        "        output_text = self.output_texts[index]\n",
        "        input_seq = text_to_tensor(input_text).squeeze(0)\n",
        "        output_seq = text_to_tensor(output_text).squeeze(0)\n",
        "        input_seq = pad_sequences([input_seq], max_len=self.max_len).squeeze(0)\n",
        "        output_seq = pad_sequences([output_seq], max_len=self.max_len).squeeze(0)\n",
        "\n",
        "        return input_seq, output_seq\n",
        "\n",
        "\n",
        "\n",
        "def tensor_to_text(tensor):\n",
        "    text = ''.join([chr(c.item()-1) if 0 <= c.item()-1 <= 0x10FFFF else '' for c in tensor])\n",
        "    return text\n",
        "\n",
        "\n",
        "def text_to_tensor(text, max_len=None):\n",
        "    seq = [ord(c) + 1 for c in text]\n",
        "    tensor = torch.tensor(seq)\n",
        "    tensor = pad_sequences([tensor], max_len=max_len).squeeze(0)\n",
        "    return tensor.unsqueeze(0)\n",
        "\n",
        "def pad_sequences(sequences, max_len=None, padding_value=0):\n",
        "    if max_len is None:\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "    padded_sequences = torch.full((len(sequences), max_len), padding_value)\n",
        "    for i, seq in enumerate(sequences):\n",
        "        padded_sequences[i, :len(seq)] = torch.tensor(seq)\n",
        "    return padded_sequences\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Extracting inputs and outputs from batch\n",
        "    input_seqs = [item[0] for item in batch]\n",
        "    target_seqs = [item[1] for item in batch]\n",
        "\n",
        "    # Find sequences with longest sequence length\n",
        "    input_seqs = pad_sequences(input_seqs, max_len=max_len)\n",
        "    target_seqs = pad_sequences(target_seqs, max_len=max_len)\n",
        "\n",
        "    return input_seqs, target_seqs\n",
        "\n",
        "\n",
        "\n",
        "input_size = 128\n",
        "hidden_size = 256\n",
        "output_size = 128\n",
        "learning_rate = 0.001\n",
        "num_epochs = 150\n",
        "\n",
        "\n",
        "translator = Translator(input_size, hidden_size, output_size)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(translator.parameters(), lr=learning_rate)\n",
        "\n",
        "dataset = TranslationDataset(input_texts, output_texts,max_len=max_len)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True,drop_last=True,collate_fn=collate_fn)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for input_seq, target_seq in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = translator(input_seq)\n",
        "        loss = criterion(output.view(-1, output_size), target_seq.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "txrxQzTSOnk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5e2589-b8e5-4044-97cc-a0b1de09d2e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/150], Loss: 1.991115927696228\n",
            "Epoch [20/150], Loss: 0.8571838140487671\n",
            "Epoch [30/150], Loss: 0.9578377604484558\n",
            "Epoch [40/150], Loss: 0.221352219581604\n",
            "Epoch [50/150], Loss: 0.10890562832355499\n",
            "Epoch [60/150], Loss: 0.04212278500199318\n",
            "Epoch [70/150], Loss: 0.26683688163757324\n",
            "Epoch [80/150], Loss: 0.010468418709933758\n",
            "Epoch [90/150], Loss: 0.19642940163612366\n",
            "Epoch [100/150], Loss: 0.013356350362300873\n",
            "Epoch [110/150], Loss: 0.011202382855117321\n",
            "Epoch [120/150], Loss: 0.015830423682928085\n",
            "Epoch [130/150], Loss: 0.004458738956600428\n",
            "Epoch [140/150], Loss: 0.007111417595297098\n",
            "Epoch [150/150], Loss: 0.0032454091124236584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input=\"Ali?\"\n",
        "test_input_seq = text_to_tensor(input,max_len=max_len)\n",
        "translator.eval()\n",
        "with torch.no_grad():\n",
        "    output = translator(test_input_seq)\n",
        "\n",
        "    k = 1  # تعداد بهترین پیش‌بینی‌ها که می‌خواهید انتخاب کنید\n",
        "    _, topk_indices = torch.topk(output, k, dim=2)\n",
        "    predicted_output_seq = topk_indices.squeeze(0)\n",
        "      \n",
        "    predicted_output_text = tensor_to_text(predicted_output_seq.squeeze())\n",
        "\n",
        "    print(\"Input: {}\".format(input))\n",
        "    print(\"Predicted Output:\", predicted_output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCy2XF6uiNwM",
        "outputId": "851eb926-7161-41fd-9eb8-f10f6077e5e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Ali?\n",
            "Predicted Output: Hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two"
      ],
      "metadata": {
        "id": "ysHQgjwExlQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "input_texts = [\n",
        "    \"چطوری؟\",\n",
        "    \"اسمت چیه؟\",\n",
        "    \"چکار می‌کنی؟\",\n",
        "    \"علی؟\"\n",
        "]\n",
        "\n",
        "output_texts = [\n",
        "    \"من خوبم، ممنون.\",\n",
        "    \"اسم من جان است.\",\n",
        "    \"من مهندس هستم.\",\n",
        "    \"سلام\"\n",
        "]\n",
        "\n",
        "# سایر بخش‌های کد\n",
        "\n"
      ],
      "metadata": {
        "id": "Bf-2YIBwxm8h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# global max_len\n",
        "max_len = max(max(len(seq) for seq in input_texts), max(len(seq) for seq in output_texts))\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "\n",
        "class Translator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Translator, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.encoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        # print(input_seq.shape)\n",
        "        embedded = self.embedding(input_seq)\n",
        "        encoder_output, (encoder_hidden, encoder_cell) = self.encoder(embedded)\n",
        "        \n",
        "        decoder_output, _ = self.decoder(encoder_output, (encoder_hidden, encoder_cell))\n",
        "\n",
        "        output = self.fc(decoder_output)\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, input_texts, output_texts, max_len):\n",
        "        self.input_texts = input_texts\n",
        "        self.output_texts = output_texts\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_text = self.input_texts[index]\n",
        "        output_text = self.output_texts[index]\n",
        "        input_seq = text_to_tensor(input_text, self.max_len).squeeze(0)\n",
        "        output_seq = text_to_tensor(output_text, self.max_len).squeeze(0)\n",
        "\n",
        "        return input_seq, output_seq\n",
        "\n",
        "\n",
        "def text_to_tensor(text, max_len=None):\n",
        "    encoded = tokenizer.encode(text, add_special_tokens=True, padding='max_length', max_length=max_len)\n",
        "    tensor = torch.tensor(encoded)\n",
        "    return tensor.unsqueeze(0)\n",
        "\n",
        "def tensor_to_text(tensor):\n",
        "    text = tokenizer.decode(tensor)\n",
        "    text = text.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").replace(\"[PAD]\", \"\")\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, max_len=None, padding_value=0):\n",
        "    if max_len is None:\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "    padded_sequences = torch.full((len(sequences), max_len), padding_value)\n",
        "    for i, seq in enumerate(sequences):\n",
        "        padded_sequences[i, :len(seq)] = torch.tensor(seq)\n",
        "    return padded_sequences\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Extracting inputs and outputs from batch\n",
        "    input_seqs = [item[0] for item in batch]\n",
        "    target_seqs = [item[1] for item in batch]\n",
        "\n",
        "    # Find sequences with longest sequence length\n",
        "    input_seqs = pad_sequences(input_seqs, max_len=max_len)\n",
        "    target_seqs = pad_sequences(target_seqs, max_len=max_len)\n",
        "\n",
        "    return input_seqs, target_seqs\n",
        "\n",
        "\n",
        "input_size = tokenizer.vocab_size\n",
        "hidden_size = 256\n",
        "output_size = tokenizer.vocab_size\n",
        "learning_rate = 0.001\n",
        "num_epochs = 150\n",
        "\n",
        "\n",
        "translator = Translator(input_size, hidden_size, output_size)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(translator.parameters(), lr=learning_rate)\n",
        "\n",
        "dataset = TranslationDataset(input_texts, output_texts,max_len=max_len)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True,drop_last=True,collate_fn=collate_fn)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for input_seq, target_seq in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = translator(input_seq)\n",
        "        loss = criterion(output.view(-1, output_size), target_seq.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hzH8t3NxrhQ",
        "outputId": "c0191fe0-7b27-4e1e-d505-4aab790309d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/150], Loss: 1.3462210893630981\n",
            "Epoch [20/150], Loss: 0.7009117007255554\n",
            "Epoch [30/150], Loss: 0.6859925389289856\n",
            "Epoch [40/150], Loss: 0.48846742510795593\n",
            "Epoch [50/150], Loss: 0.4069752097129822\n",
            "Epoch [60/150], Loss: 0.04743042215704918\n",
            "Epoch [70/150], Loss: 0.21529562771320343\n",
            "Epoch [80/150], Loss: 0.15566202998161316\n",
            "Epoch [90/150], Loss: 0.24217753112316132\n",
            "Epoch [100/150], Loss: 0.013216380029916763\n",
            "Epoch [110/150], Loss: 0.04095226526260376\n",
            "Epoch [120/150], Loss: 0.007901791483163834\n",
            "Epoch [130/150], Loss: 0.006592517718672752\n",
            "Epoch [140/150], Loss: 0.005630132742226124\n",
            "Epoch [150/150], Loss: 0.01746489852666855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input=\"چکار می‌کنی؟\"\n",
        "test_input_seq = text_to_tensor(input,max_len=max_len)\n",
        "translator.eval()\n",
        "with torch.no_grad():\n",
        "    output = translator(test_input_seq)\n",
        "\n",
        "    k = 1  # تعداد بهترین پیش‌بینی‌ها که می‌خواهید انتخاب کنید\n",
        "    _, topk_indices = torch.topk(output, k, dim=2)\n",
        "    predicted_output_seq = topk_indices.squeeze(0)\n",
        "      \n",
        "    predicted_output_text = tensor_to_text(predicted_output_seq.squeeze())\n",
        "\n",
        "    print(\"Input: {}\".format(input))\n",
        "    print(\"Predicted Output:\", predicted_output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGD8PSelxpTs",
        "outputId": "2e126803-3377-4afe-9178-c62967185c5e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: چکار می‌کنی؟\n",
            "Predicted Output:  من مهندس هستم.          \n"
          ]
        }
      ]
    }
  ]
}