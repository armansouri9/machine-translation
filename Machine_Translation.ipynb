{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Dataset\n",
        "input_texts = [\n",
        "    \"How are you?\",\n",
        "    \"What's your name?\",\n",
        "    \"What do you do?\",\n",
        "    \"Ali?\"\n",
        "]\n",
        "\n",
        "output_texts = [\n",
        "     \"I'm fine, thank you.\",\n",
        "    \"My name is John.\",\n",
        "    \"i'm engineer\",\n",
        "    \"reza\"\n",
        "]"
      ],
      "metadata": {
        "id": "YVYQBPIlhOEI"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Translator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Translator, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.encoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "\n",
        "        embedded = self.embedding(input_seq)\n",
        "        encoder_output, (encoder_hidden, encoder_cell) = self.encoder(embedded)\n",
        "        \n",
        "        decoder_output, _ = self.decoder(encoder_output, (encoder_hidden, encoder_cell))\n",
        "\n",
        "        output = self.fc(decoder_output)\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, input_texts, output_texts):\n",
        "        self.input_texts = input_texts\n",
        "        self.output_texts = output_texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_text = self.input_texts[index]\n",
        "        output_text = self.output_texts[index]\n",
        "        input_seq = text_to_tensor(input_text).squeeze(0)\n",
        "        output_seq = text_to_tensor(output_text).squeeze(0)\n",
        "\n",
        "        # pad input_seq and output_seq\n",
        "        max_len = max(input_seq.shape[0], output_seq.shape[0])\n",
        "        input_seq = pad_sequences([input_seq], max_len=max_len).squeeze(0)\n",
        "        output_seq = pad_sequences([output_seq], max_len=max_len).squeeze(0)\n",
        "\n",
        "        return input_seq, output_seq\n",
        "\n",
        "\n",
        "\n",
        "def tensor_to_text(tensor):\n",
        "    text = ''.join([chr(c.item()-1) if 0 <= c.item()-1 <= 0x10FFFF else '' for c in tensor])\n",
        "    return text\n",
        "\n",
        "\n",
        "def text_to_tensor(text):\n",
        "    seq = [ord(c)+1 for c in text]\n",
        "    tensor = torch.tensor(seq)\n",
        "    return tensor.unsqueeze(0)\n",
        "\n",
        "def pad_sequences(sequences, max_len=None, padding_value=0):\n",
        "    if max_len is None:\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "    padded_sequences = torch.full((len(sequences), max_len), padding_value)\n",
        "    for i, seq in enumerate(sequences):\n",
        "        padded_sequences[i, :len(seq)] = torch.tensor(seq)\n",
        "    return padded_sequences\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Extracting inputs and outputs from batch\n",
        "    input_seqs = [item[0] for item in batch]\n",
        "    target_seqs = [item[1] for item in batch]\n",
        "\n",
        "    # Find the length of the longest sequence in batch\n",
        "    max_len = max(max(len(seq) for seq in input_seqs), max(len(seq) for seq in target_seqs))\n",
        "\n",
        "    # Find sequences with longest sequence length\n",
        "    input_seqs = pad_sequences(input_seqs, max_len=max_len)\n",
        "    target_seqs = pad_sequences(target_seqs, max_len=max_len)\n",
        "\n",
        "    return input_seqs, target_seqs\n",
        "\n",
        "\n",
        "input_size = 128\n",
        "hidden_size = 256\n",
        "output_size = 128\n",
        "learning_rate = 0.001\n",
        "num_epochs = 150\n",
        "\n",
        "\n",
        "translator = Translator(input_size, hidden_size, output_size)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(translator.parameters(), lr=learning_rate)\n",
        "\n",
        "dataset = TranslationDataset(input_texts, output_texts)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True,drop_last=True,collate_fn=collate_fn)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for input_seq, target_seq in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = translator(input_seq)\n",
        "        # print(output.view(-1, output_size).shape)\n",
        "        # print(target_seq.view(-1).shape)\n",
        "        loss = criterion(output.view(-1, output_size), target_seq.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "txrxQzTSOnk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeee4c52-8b91-47d3-9047-d04aa23b882a"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-200-f73f8c367c91>:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_sequences[i, :len(seq)] = torch.tensor(seq)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/150], Loss: 2.1001980304718018\n",
            "Epoch [20/150], Loss: 0.062242113053798676\n",
            "Epoch [30/150], Loss: 0.22997207939624786\n",
            "Epoch [40/150], Loss: 0.3844526410102844\n",
            "Epoch [50/150], Loss: 0.3091491758823395\n",
            "Epoch [60/150], Loss: 0.22025592625141144\n",
            "Epoch [70/150], Loss: 0.004945266526192427\n",
            "Epoch [80/150], Loss: 0.01911712996661663\n",
            "Epoch [90/150], Loss: 0.015034961514174938\n",
            "Epoch [100/150], Loss: 0.04524823650717735\n",
            "Epoch [110/150], Loss: 0.03369804471731186\n",
            "Epoch [120/150], Loss: 0.007843194529414177\n",
            "Epoch [130/150], Loss: 0.02131364867091179\n",
            "Epoch [140/150], Loss: 0.006317059975117445\n",
            "Epoch [150/150], Loss: 0.0052369749173521996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_seq = text_to_tensor(\"Ali!\")\n",
        "translator.eval()\n",
        "with torch.no_grad():\n",
        "    output = translator(test_input_seq)\n",
        "    \n",
        "    \n",
        "    k = 1  # تعداد بهترین پیش‌بینی‌ها که می‌خواهید انتخاب کنید\n",
        "    _, topk_indices = torch.topk(output, k, dim=2)\n",
        "    predicted_output_seq = topk_indices.squeeze(0)\n",
        "      \n",
        "    # predicted_output_seq = torch.argmax(output, dim=2)\n",
        "    predicted_output_text = tensor_to_text(predicted_output_seq.squeeze())\n",
        "\n",
        "    print(\"Input: Ali?\")\n",
        "    print(\"Predicted Output:\", predicted_output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCy2XF6uiNwM",
        "outputId": "22c943b4-f910-4d03-ebc1-ea082ac8fcda"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Ali?\n",
            "Predicted Output: reza\n"
          ]
        }
      ]
    }
  ]
}